# -*- coding: utf-8 -*-
import os
import sys
import json
import cv2
import numpy as np
import pandas as pd
from PIL import Image
from deepface import DeepFace
from scipy.spatial.distance import cosine, euclidean
from concurrent.futures import ThreadPoolExecutor

# âœ… Firestore èªè¨¼è¿½åŠ 
import firebase_admin
from firebase_admin import credentials, firestore

# ğŸ”¹ Firebase èªè¨¼ï¼ˆé©åˆ‡ãªãƒ‘ã‚¹ã«å¤‰æ›´ï¼‰
cred = credentials.Certificate("./firebase-admin-key.json")  # â† ã“ã“ã‚’æ­£ã—ã„ãƒ‘ã‚¹ã«
firebase_admin.initialize_app(cred)
db = firestore.client()

# âœ… TensorFlow ã® oneDNN æœ€é©åŒ–ã‚’ç„¡åŠ¹åŒ–
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

# ğŸ”¹ é¡”æ¤œå‡º + åˆ‡ã‚ŠæŠœã + ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†
def process_face(image_path, output_path, target_size=(160, 160)):
    try:
        faces = DeepFace.extract_faces(image_path, detector_backend="ssd", enforce_detection=False)
        if not faces:
            return {"error": f"No face detected in {image_path}"}
        face = (faces[0]["face"] * 255).astype(np.uint8)
        h, w, _ = face.shape
        max_dim = max(h, w)
        padded = np.ones((max_dim, max_dim, 3), dtype=np.uint8) * 255
        sx = (max_dim - w) // 2
        sy = (max_dim - h) // 2
        padded[sy:sy + h, sx:sx + w] = face
        resized = cv2.resize(padded, target_size)
        Image.fromarray(resized).save(output_path)
        return output_path
    except Exception as e:
        return {"error": f"Face processing failed: {str(e)}"}

# ğŸ”¹ Firestoreã‹ã‚‰è¿½åŠ ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
def fetch_member_info(image_name):
    docs = db.collection("members").where("imageNames", "array_contains", image_name).stream()
    for doc in docs:
        data = doc.to_dict()
        return {
            "name": data.get("name"),
            "group": data.get("group"),
            "age": data.get("age"),
            "imageUrl": data.get("imageUrl")
        }
    return {
        "name": None,
        "group": None,
        "age": None,
        "imageUrl": None
    }

# ğŸ”¹ å…¥åŠ›ç”»åƒã®å‡¦ç†
image_paths = sys.argv[1:]
if not image_paths:
    print(json.dumps({"error": "No images provided"}))
    sys.exit(1)

processed_images = []
for idx, image_path in enumerate(image_paths):
    output_path = f"cropped_user_{idx}.jpg"
    result = process_face(image_path, output_path)
    if isinstance(result, dict) and "error" in result:
        print(json.dumps(result))
        sys.exit(1)
    processed_images.append(output_path)

# ğŸ”¹ ç‰¹å¾´é‡æŠ½å‡ºï¼ˆ4096æ¬¡å…ƒï¼‰
def extract_features(image_path):
    return DeepFace.represent(
        img_path=image_path,
        model_name="VGG-Face",
        detector_backend="skip",
        enforce_detection=False
    )[0]['embedding']

with ThreadPoolExecutor(max_workers=1) as executor:
    features_list = list(executor.map(extract_features, processed_images))

if not features_list:
    print(json.dumps({"error": "No valid features extracted"}))
    sys.exit(1)

user_features = np.median(np.vstack(features_list), axis=0)

# ğŸ”¹ ãƒ¡ãƒ³ãƒãƒ¼ç‰¹å¾´é‡ã®èª­ã¿è¾¼ã¿
FEATURES_CSV = "member_features_vggface_direct.csv"
df = pd.read_csv(FEATURES_CSV)
feature_cols = [f"v{i+1}" for i in range(4096)]
member_matrix = df[feature_cols].values

if df.empty:
    print(json.dumps({"error": "No member data found in CSV"}))
    sys.exit(1)

# ğŸ”¹ é¡ä¼¼åº¦è¨ˆç®—
df["cosine_similarity"] = 1 - np.apply_along_axis(lambda x: cosine(user_features, x), 1, member_matrix)
df["euclidean_distance"] = np.linalg.norm(member_matrix - user_features, axis=1)
df["similarity_score"] = (1.2 * df["cosine_similarity"]) - (0.8 * df["euclidean_distance"])
df_sorted = df.sort_values(by="similarity_score", ascending=False).head(10)

# ğŸ”¹ Firestoreã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’è£œå®Œ
output = []
for _, row in df_sorted.iterrows():
    info = fetch_member_info(row["image_name"])
    result = {
        "image_name": row["image_name"],
        "cosine_similarity": row["cosine_similarity"],
        "euclidean_distance": row["euclidean_distance"],
        "similarity_score": row["similarity_score"],
        **info
    }
    output.append(result)

# ğŸ”¹ çµæœã‚’JSONã§å‡ºåŠ›
print(json.dumps(output, ensure_ascii=False))
